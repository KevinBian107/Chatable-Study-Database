{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Analysis\n",
    "You should create an seperate `yaml` for each of the project you are working on, this is a good practice in general. Thesre migt be weird issues that causes a dependency problem. For the same reason a new `yaml` file would record all the dependencies that would ensure this pipeline works for future references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.keys()\n",
    "pio.renderers.default = 'notebook' \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from utils.eda import *\n",
    "from utils.text_model import transform_text, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text data\n",
    "fall_2022_text = pd.read_csv('data/2022_fall_text.csv')\n",
    "winter_2023_text = pd.read_csv('data/2023_winter_text.csv')\n",
    "spring_2023_text = pd.read_csv('data/2023_spring_text.csv')\n",
    "summer_2023_text = pd.read_csv('data/2023_summer_text.csv')\n",
    "fall_2023_text = pd.read_csv('data/2023_fall_text.csv')\n",
    "winter_2024_text = pd.read_csv('data/2024_winter_text.csv')\n",
    "spring_2024_text = pd.read_csv('data/2024_spring_text.csv')\n",
    "summer_2024_text = pd.read_csv('data/2024_summer_text.csv')\n",
    "fall_2024_text = pd.read_csv('data/2024_fall_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Study Materials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-14 | Book read (30m), DOC 2 P3 part2 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-15 | Nutrition read (15m), Physics Lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-21 |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-22 | After consecutively doing DSC 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-23 | Wolfram good to check solution | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2024-12-11 | RPLH eval + paper algorithm + RPL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2024-12-12 | track-mjx meeting, RPLH paper wri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2024-12-13 | constraint article, EM article, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2024-12-14 | chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2024-12-15 | training, rplh summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       Study Materials\n",
       "0    2022-02-14 | Book read (30m), DOC 2 P3 part2 (...\n",
       "1    2022-02-15 | Nutrition read (15m), Physics Lig...\n",
       "2                                        2022-10-21 | \n",
       "3    2022-10-22 | After consecutively doing DSC 10 ...\n",
       "4    2022-10-23 | Wolfram good to check solution | ...\n",
       "..                                                 ...\n",
       "628  2024-12-11 | RPLH eval + paper algorithm + RPL...\n",
       "629  2024-12-12 | track-mjx meeting, RPLH paper wri...\n",
       "630  2024-12-13 | constraint article, EM article, m...\n",
       "631                                 2024-12-14 | chill\n",
       "632                2024-12-15 | training, rplh summary\n",
       "\n",
       "[633 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Concat\n",
    "text = pd.concat([fall_2022_text, winter_2023_text, spring_2023_text, summer_2023_text, fall_2023_text, winter_2024_text, spring_2024_text, summer_2024_text, fall_2024_text], axis=0)\n",
    "clean_text = text.pipe(transform_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15955"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_process = (clean_text['Study Materials']\n",
    "              .str.lower()\n",
    "              .str.replace(r'\\([\\d]*m\\)','',regex=True)\n",
    "              .str.replace(',','')\n",
    "              .str.strip())\n",
    "\n",
    "corpus = ' '.join(pre_process.astype(str).to_list())\n",
    "tokens = nltk.tokenize.word_tokenize(corpus, language='english')\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Chat Familier With My Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either of `sentence-transformer`, `nltk`, `openai`, `langchain`, or related stuff has many dependency issue if just pyt in a big environment, need to have a seperated contained environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import ast\n",
    "import openai\n",
    "import os\n",
    "\n",
    "nltk.download('punkt')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks created: 633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 20/20 [00:00<00:00, 35.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings.csv\n"
     ]
    }
   ],
   "source": [
    "from utils.text_model import split_text_nltk, get_similar_chunks, generate_response\n",
    "\n",
    "documents = clean_text['Study Materials'].tolist()\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = split_text_nltk(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "embeddings = model.encode(all_chunks, show_progress_bar=True, convert_to_tensor=False)\n",
    "\n",
    "embedding_df = pd.DataFrame({\n",
    "    'chunk': all_chunks,\n",
    "    'embedding': embeddings.tolist()\n",
    "})\n",
    "\n",
    "embedding_df.to_csv('embeddings.csv', index=False)\n",
    "print(\"Embeddings saved to embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index has 633 vectors.\n"
     ]
    }
   ],
   "source": [
    "embedding_df = pd.read_csv('embeddings.csv')\n",
    "embedding_df['embedding'] = embedding_df['embedding'].apply(ast.literal_eval)\n",
    "\n",
    "# Convert embeddings to a NumPy array of type float32\n",
    "embeddings = np.array(embedding_df['embedding'].tolist()).astype('float32')\n",
    "\n",
    "# Initialize FAISS index and using L2 distance, can also use cosine similarity\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add embeddings to the index\n",
    "index.add(embeddings)\n",
    "print(f\"FAISS index has {index.ntotal} vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-14 | Book read (30m), DOC 2 P3 part2 (...</td>\n",
       "      <td>[-0.02826680615544319, -0.004485192708671093, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-15 | Nutrition read (15m), Physics Lig...</td>\n",
       "      <td>[-0.06095949560403824, -0.054127417504787445, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-21 |</td>\n",
       "      <td>[-0.09296953678131104, 0.028568459674715996, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-22 | After consecutively doing DSC 10 ...</td>\n",
       "      <td>[-0.08834125101566315, -0.009416475892066956, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-23 | Wolfram good to check solution | ...</td>\n",
       "      <td>[-0.05123080313205719, 0.03428500518202782, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2024-12-11 | RPLH eval + paper algorithm + RPL...</td>\n",
       "      <td>[-0.09284821152687073, 0.07178058475255966, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2024-12-12 | track-mjx meeting, RPLH paper wri...</td>\n",
       "      <td>[-0.04259365424513817, 0.030164869502186775, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2024-12-13 | constraint article, EM article, m...</td>\n",
       "      <td>[-0.10550352931022644, 0.030236540362238884, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2024-12-14 | chill</td>\n",
       "      <td>[-0.0889950692653656, 0.03961106017231941, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2024-12-15 | training, rplh summary</td>\n",
       "      <td>[-0.046767979860305786, 0.03863947466015816, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 chunk  \\\n",
       "0    2022-02-14 | Book read (30m), DOC 2 P3 part2 (...   \n",
       "1    2022-02-15 | Nutrition read (15m), Physics Lig...   \n",
       "2                                         2022-10-21 |   \n",
       "3    2022-10-22 | After consecutively doing DSC 10 ...   \n",
       "4    2022-10-23 | Wolfram good to check solution | ...   \n",
       "..                                                 ...   \n",
       "628  2024-12-11 | RPLH eval + paper algorithm + RPL...   \n",
       "629  2024-12-12 | track-mjx meeting, RPLH paper wri...   \n",
       "630  2024-12-13 | constraint article, EM article, m...   \n",
       "631                                 2024-12-14 | chill   \n",
       "632                2024-12-15 | training, rplh summary   \n",
       "\n",
       "                                             embedding  \n",
       "0    [-0.02826680615544319, -0.004485192708671093, ...  \n",
       "1    [-0.06095949560403824, -0.054127417504787445, ...  \n",
       "2    [-0.09296953678131104, 0.028568459674715996, 0...  \n",
       "3    [-0.08834125101566315, -0.009416475892066956, ...  \n",
       "4    [-0.05123080313205719, 0.03428500518202782, -0...  \n",
       "..                                                 ...  \n",
       "628  [-0.09284821152687073, 0.07178058475255966, 0....  \n",
       "629  [-0.04259365424513817, 0.030164869502186775, 0...  \n",
       "630  [-0.10550352931022644, 0.030236540362238884, 0...  \n",
       "631  [-0.0889950692653656, 0.03961106017231941, 0.0...  \n",
       "632  [-0.046767979860305786, 0.03863947466015816, 0...  \n",
       "\n",
       "[633 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"...\" #os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In the 2022 fall quarter, you mainly spent time on various tasks such as checking and documenting your spending for Christmas break and the BOA 2022 fall quarter, working on projects such as the Personal Study Time Analysis Project, Cogs 9 Final Project, DOC 1 Portfolio 4, and Math 18 大局观, and meeting new people.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What did I mainly do in 2022 fall quarter?\"\n",
    "similar_chunks = get_similar_chunks(user_prompt, index, embedding_df, top_k=5)\n",
    "generate_response(user_prompt, similar_chunks, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In 2023 fall quarter, I mainly did housework, planning, and training during the holiday break. I also spent time analyzing my spending and documenting it for Bank of America's 2022 fall quarter. I had a break on Christmas and New Year's Day. I also worked on various projects and submitted them, including a personal study time analysis project, a Cogs 9 final project, a DOC 1 portfolio, and a 大局观 for Math 18. I also had the opportunity to meet fun people during this time.\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What did I mainly do in 2023 fall quarter?\"\n",
    "similar_chunks = get_similar_chunks(user_prompt, index, embedding_df, top_k=5)\n",
    "generate_response(user_prompt, similar_chunks, api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In 2024 fall quarter, you spent time cleaning your house, updating your resume and website, and analyzing study time data. You also spent time running and training your legs, and enjoyed some leisure activities. Additionally, you spent time working on a project for your Github repository.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"What did I mainly do in 2024 fall quarter?\"\n",
    "similar_chunks = get_similar_chunks(user_prompt, index, embedding_df, top_k=5)\n",
    "generate_response(user_prompt, similar_chunks, api_key=api_key)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
