{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.renderers.keys()\n",
    "pio.renderers.default = 'notebook' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/kevinb/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "from itertools import chain\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from utils.eda import *\n",
    "from utils.text_model import transform_text, tokenize\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text data\n",
    "fall_2022_text = pd.read_csv('data/2022_fall_text.csv')\n",
    "winter_2023_text = pd.read_csv('data/2023_winter_text.csv')\n",
    "spring_2023_text = pd.read_csv('data/2023_spring_text.csv')\n",
    "summer_2023_text = pd.read_csv('data/2023_summer_text.csv')\n",
    "fall_2023_text = pd.read_csv('data/2023_fall_text.csv')\n",
    "winter_2024_text = pd.read_csv('data/2024_winter_text.csv')\n",
    "spring_2024_text = pd.read_csv('data/2024_spring_text.csv')\n",
    "summer_2024_text = pd.read_csv('data/2024_summer_text.csv')\n",
    "fall_2024_text = pd.read_csv('data/2024_fall_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Study Materials</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-14</td>\n",
       "      <td>Book read (30m), DOC 2 P3 part2 (30m), Terms o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-15</td>\n",
       "      <td>Nutrition read (15m), Physics Light cone study...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-21</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>After consecutively doing DSC 10 hw, efficienc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>Wolfram good to check solution | Group  Coding...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>628</th>\n",
       "      <td>2024-12-11</td>\n",
       "      <td>RPLH eval + paper algorithm + RPLH eval, track...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629</th>\n",
       "      <td>2024-12-12</td>\n",
       "      <td>track-mjx meeting, RPLH paper writing, chat, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2024-12-13</td>\n",
       "      <td>constraint article, EM article, meet elliot,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>631</th>\n",
       "      <td>2024-12-14</td>\n",
       "      <td>chill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>632</th>\n",
       "      <td>2024-12-15</td>\n",
       "      <td>training, rplh summary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>633 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Time                                    Study Materials\n",
       "0   2022-02-14  Book read (30m), DOC 2 P3 part2 (30m), Terms o...\n",
       "1   2022-02-15  Nutrition read (15m), Physics Light cone study...\n",
       "2   2022-10-21                                                   \n",
       "3   2022-10-22  After consecutively doing DSC 10 hw, efficienc...\n",
       "4   2022-10-23  Wolfram good to check solution | Group  Coding...\n",
       "..         ...                                                ...\n",
       "628 2024-12-11  RPLH eval + paper algorithm + RPLH eval, track...\n",
       "629 2024-12-12  track-mjx meeting, RPLH paper writing, chat, c...\n",
       "630 2024-12-13  constraint article, EM article, meet elliot,  ...\n",
       "631 2024-12-14                                              chill\n",
       "632 2024-12-15                             training, rplh summary\n",
       "\n",
       "[633 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initial Concat\n",
    "text = pd.concat([fall_2022_text, winter_2023_text, spring_2023_text, summer_2023_text, fall_2023_text, winter_2024_text, spring_2024_text, summer_2024_text, fall_2024_text], axis=0)\n",
    "clean_text = text.pipe(transform_text)\n",
    "clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_process = (clean_text['Study Materials']\n",
    "#               .str.lower()\n",
    "#               .str.replace(r'\\([\\d]*m\\)','',regex=True)\n",
    "#               .str.replace(',','')\n",
    "#               .str.strip())\n",
    "\n",
    "# corpus = ' '.join(pre_process.astype(str).to_list())\n",
    "# tokens = nltk.tokenize.word_tokenize(corpus, language='english')\n",
    "# len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_C' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Download NLTK data files (if not already downloaded)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/ucsd_study/lib/python3.8/site-packages/sentence_transformers/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m export_dynamic_quantized_onnx_model, export_optimized_onnx_model\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_encoder\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParallelSentencesDataset, SentencesDataset\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mLoggingHandler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoggingHandler\n",
      "File \u001b[0;32m~/miniforge3/envs/ucsd_study/lib/python3.8/site-packages/sentence_transformers/cross_encoder/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCrossEncoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CrossEncoder\n\u001b[1;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCrossEncoder\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/envs/ucsd_study/lib/python3.8/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable, Literal, overload\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tensor, nn\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer\n",
      "File \u001b[0;32m~/miniforge3/envs/ucsd_study/lib/python3.8/site-packages/torch/__init__.py:764\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m  \u001b[38;5;66;03m# If __file__ is not None the cause is unknown, so just re-raise.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m __name, __obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m __name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[43m_C\u001b[49m):\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m __name[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m __name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBase\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    766\u001b[0m         __all__\u001b[38;5;241m.\u001b[39mappend(__name)\n",
      "\u001b[0;31mNameError\u001b[0m: name '_C' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# Download NLTK data files (if not already downloaded)\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Initialize Sentence Transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # You can choose other models as needed\n",
    "\n",
    "# Load your DataFrame\n",
    "df = pd.read_csv('your_data.csv')  # Replace with your actual data source\n",
    "\n",
    "# Extract the text column\n",
    "documents = df['text_column'].tolist()  # Replace 'text_column' with your actual column name\n",
    "\n",
    "# Function to split text into chunks\n",
    "def split_text_nltk(text, max_sentences=50):\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on a maximum number of sentences.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), max_sentences):\n",
    "        chunk = ' '.join(sentences[i:i + max_sentences])\n",
    "        chunks.append(chunk)\n",
    "    return chunks\n",
    "\n",
    "# Split all documents into chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    chunks = split_text_nltk(doc)\n",
    "    all_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "\n",
    "# Generate embeddings for each chunk\n",
    "embeddings = model.encode(all_chunks, show_progress_bar=True, convert_to_tensor=False)\n",
    "\n",
    "# Add embeddings to a new DataFrame\n",
    "embedding_df = pd.DataFrame({\n",
    "    'chunk': all_chunks,\n",
    "    'embedding': embeddings.tolist()\n",
    "})\n",
    "\n",
    "# Save embeddings to a file (optional)\n",
    "embedding_df.to_csv('embeddings.csv', index=False)\n",
    "print(\"Embeddings saved to embeddings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
