{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A \"Chatable\" Study Database \ud83d\udcac \u00b6 Data science in practice, I created this manually collected study data for modeling my study time in UCSD since 2022 as a Freshman. I conducted numerous numerical analysis to discover some study habit that I have and analyze when I would be busiest for later quarter's understanding. In addition, I also created chat functions with a language model for a personalized search engine. Data changes format through out different quarters, becoming more developed and suitable, so merging and some cleaning is needed at first. Each quarter's data includes one data frame of all the study/work time data and an text feature data frame for the work conducted: One almost fully Timestamp + Numerical data frame ( year_quarter_study ) that records all the study_time One almost fully Timestamp + Text data frame ( year_quarter_text ) that records the precise study_subject Data currently include: 2022_fall_study.csv + 2022_fall_text.csv 2022_winter_study.csv + 2022_winter_text.csv 2022_spring_study.csv + 2022_spring_text.csv 2022_summer_study.csv + 2022_summer_text.csv 2023_fall_study.csv + 2023_fall_text.csv 2024_winter_study.csv + 2024_winter_text.csv 2024_spring_study.csv + 2024_spring_text.csv 2024_summer_study.csv + 2024_summer_text.csv 2024_fall_study.csv + 2024_fall_text.csv Currently I did: Created a chating function where an language model would take all the text in the csv files as embedding and build a personalized search engine. Conducted explorative data analysis with numerical data, specifcally timestamp data.","title":"Study Database"},{"location":"#a-chatable-study-database","text":"Data science in practice, I created this manually collected study data for modeling my study time in UCSD since 2022 as a Freshman. I conducted numerous numerical analysis to discover some study habit that I have and analyze when I would be busiest for later quarter's understanding. In addition, I also created chat functions with a language model for a personalized search engine. Data changes format through out different quarters, becoming more developed and suitable, so merging and some cleaning is needed at first. Each quarter's data includes one data frame of all the study/work time data and an text feature data frame for the work conducted: One almost fully Timestamp + Numerical data frame ( year_quarter_study ) that records all the study_time One almost fully Timestamp + Text data frame ( year_quarter_text ) that records the precise study_subject Data currently include: 2022_fall_study.csv + 2022_fall_text.csv 2022_winter_study.csv + 2022_winter_text.csv 2022_spring_study.csv + 2022_spring_text.csv 2022_summer_study.csv + 2022_summer_text.csv 2023_fall_study.csv + 2023_fall_text.csv 2024_winter_study.csv + 2024_winter_text.csv 2024_spring_study.csv + 2024_spring_text.csv 2024_summer_study.csv + 2024_summer_text.csv 2024_fall_study.csv + 2024_fall_text.csv Currently I did: Created a chating function where an language model would take all the text in the csv files as embedding and build a personalized search engine. Conducted explorative data analysis with numerical data, specifcally timestamp data.","title":"A \"Chatable\" Study Database \ud83d\udcac"},{"location":"api/","text":"Setting Up & Running \"Chat\" Function on Database \u00b6 Create the conda enviornment by: conda env create If you want to use GPT with API, you need to create your own OpenAI account and then embed your API key in your system with writing this in your .bash file: export OPENAI_API_KEY = \"your api key\" Run the following to update system file: source ~/. bash_profile Enter the conda environment conda activate ucsd_study Then run an instance ( chat_with_feedback ) of our chat function by: python chat / chat_with_feedback . py We have created a few versions of our chat functions: - chat_base.py is the vanill implementation of the chat function. - chat_langchain.py atampts to us the langchain package ( not working yet ). - chat_standard.py is the currently useful standard version. - chat_with_feedcack.py is chat_standard.py but implemented a feedcak for follow up questions, which is much smarter and useful than the standard version. An example of chat feedback in in here and we have a demo of chat function in here: Your browser does not support the video tag.","title":"API Calls"},{"location":"api/#setting-up-running-chat-function-on-database","text":"Create the conda enviornment by: conda env create If you want to use GPT with API, you need to create your own OpenAI account and then embed your API key in your system with writing this in your .bash file: export OPENAI_API_KEY = \"your api key\" Run the following to update system file: source ~/. bash_profile Enter the conda environment conda activate ucsd_study Then run an instance ( chat_with_feedback ) of our chat function by: python chat / chat_with_feedback . py We have created a few versions of our chat functions: - chat_base.py is the vanill implementation of the chat function. - chat_langchain.py atampts to us the langchain package ( not working yet ). - chat_standard.py is the currently useful standard version. - chat_with_feedcack.py is chat_standard.py but implemented a feedcak for follow up questions, which is much smarter and useful than the standard version. An example of chat feedback in in here and we have a demo of chat function in here: Your browser does not support the video tag.","title":"Setting Up &amp; Running \"Chat\" Function on Database"},{"location":"eda/","text":"Conducting explorative analysis on the numerical data. Mainly focuses on looking at changes in study habits reflected in study times. Temporal Analysis: \u00b6 From the first plot, we are looking at some overall temporal trend and how my study hours flunctuate in different week days, different quarters, and different years. Looks like study time overall increases since Freshman. The monthly pattern in the second plot is consistent for each year where summer time has significantly less work loads and th erest flunctuates about the same. We can also look at some specific categories of what I do, specifcally speaking ( research , dsc , math , and cogs ). There seems to be some category specific trend, spending much moretimes on both research stuff and data science stuff. We can also examine how study hours changes as a function of week-days and week-number examine seperately by each year . Seems to be the busiest around week 30. Seems to be usually busy aorund Tuesday and Wednesday and chilling on the weekend. At last, let's look at each season 's effect on study times by using a heatmap. We can see that there is an overall increase in study time over the years and the longest cumulative study time seems to be at Fall and Spring quarter (busiest quarters). Dimensionality Reductions \u00b6 Using some dimensionality reduction technique, we can show some underlaying property of the numerical data belong to each quarter. Specifcally, we can show thatthere seems to exist different study habits, causing different clusters of data. Categorical Course Analysis \u00b6 Created an overall statistics of how each course varies on time usage, having details relative to each classes.","title":"EDA"},{"location":"eda/#temporal-analysis","text":"From the first plot, we are looking at some overall temporal trend and how my study hours flunctuate in different week days, different quarters, and different years. Looks like study time overall increases since Freshman. The monthly pattern in the second plot is consistent for each year where summer time has significantly less work loads and th erest flunctuates about the same. We can also look at some specific categories of what I do, specifcally speaking ( research , dsc , math , and cogs ). There seems to be some category specific trend, spending much moretimes on both research stuff and data science stuff. We can also examine how study hours changes as a function of week-days and week-number examine seperately by each year . Seems to be the busiest around week 30. Seems to be usually busy aorund Tuesday and Wednesday and chilling on the weekend. At last, let's look at each season 's effect on study times by using a heatmap. We can see that there is an overall increase in study time over the years and the longest cumulative study time seems to be at Fall and Spring quarter (busiest quarters).","title":"Temporal Analysis:"},{"location":"eda/#dimensionality-reductions","text":"Using some dimensionality reduction technique, we can show some underlaying property of the numerical data belong to each quarter. Specifcally, we can show thatthere seems to exist different study habits, causing different clusters of data.","title":"Dimensionality Reductions"},{"location":"eda/#categorical-course-analysis","text":"Created an overall statistics of how each course varies on time usage, having details relative to each classes.","title":"Categorical Course Analysis"}]}